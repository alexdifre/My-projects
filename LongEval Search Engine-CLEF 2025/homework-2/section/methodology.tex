\section{Methodology}
\label{sec:methodology}

In this section, we describe the methodology adopted to develop our IR system for the task.

\subsection{Parser}
The parser is responsible for reading and extracting structured data from complex document formats, enabling effective indexing and retrieval. Our document parsing framework consists of several components designed to handle different document structures efficiently:

\begin{itemize}

\item \textbf{DocumentParser:} This abstract class serves as the core of our parsing framework. It provides a template for iterating over documents. The class ensures that documents are read correctly.

\item \textbf{ParsedDocument:} This class encapsulates the structure of a parsed document. It defines essential fields such as:
  \begin{itemize}
    \item \texttt{ID} - A unique identifier for each document.
    \item \texttt{Body} - The main content of the document.
    \item \texttt{Start} - Initial lines of the document, often used for quick reference.
    \item \texttt{Highlights} - Notable words or phrases extracted from tags or special formatting, enhancing keyword extraction.
  \end{itemize}

\item \textbf{TrecParser:} This specialized class extends \texttt{DocumentParser} to handle TREC-formatted documents. Key functions include:
  \begin{itemize}
    \item Regex-based extraction of document fields such as \texttt{<DOCNO>} and \texttt{<DOCID>}.
    \item Management of document content using a multistage buffering approach to efficiently handle large text bodies.
    \item Extraction of emphasized content through patterns such as \texttt{hashtags} and \texttt{strong} tags, improving the semantic richness of stored highlights.
    \item Support for emoji and URL filtering, ensuring text data is clean and normalized before analysis.
  \end{itemize}

\item \textbf{Error Handling:} Robust error management is implemented to gracefully handle missing or malformed data, maintaining the stability and reliability of the parser.

\end{itemize}

This structured parsing methodology allows for the seamless transformation of raw text into well-defined data objects, paving the way for accurate indexing and retrieval operations.

\subsection{Analyzer}
The \texttt{FrenchAnalyzer} is designed to process French texts, integrating multiple components:

In the construction of a Lucene-based text analysis pipeline, the sequence of token filters applied to a token stream plays a critical role in shaping the effectiveness of indexing and retrieval. This document outlines the rationale behind the sequential ordering of token filters implemented in the \texttt{createComponents} method of the analyzer. Each filter serves a distinct purpose, and its placement in the pipeline is determined by its functional dependencies and intended effects on the token stream.

\textbf{ Pipeline Overview:}

The method under consideration constructs the analysis pipeline as follows:

\begin{enumerate}
    \item A tokenizer is selected based on the \texttt{tokenizerType}, producing the initial token stream.
    \item A sequence of token filters is applied in a specific order:
    \begin{enumerate}
        \item \texttt{LowerCaseFilter}
        \item Optional: \texttt{RepeatedLetterFilter}
        \item Optional: \texttt{AbbreviationExpansionFilter}
        \item \texttt{ICUFoldingFilter}
        \item \texttt{NBSPFilter}
        \item \texttt{ElisionFilter}
        \item \texttt{RemoveDuplicatesTokenFilter}
        \item Optional: \texttt{CompoundPOSTokenFilter}
        \item Optional: \texttt{ShingleFilter}
        \item Optional: \texttt{LengthFilter}
        \item \texttt{Stopword Removal}
        \item Optional: \texttt{PositionFilter}
    \end{enumerate}
\end{enumerate}


\subsection{LowerCaseFilter}
The \texttt{LowerCaseFilter} is applied immediately after tokenization to normalize case, ensuring that all subsequent filters operate on a consistent lowercase representation. This step facilitates reliable term matching and avoids case-sensitive inconsistencies in abbreviation expansion or folding.

\subsection{RepeatedLetterFilter (Optional)}
When enabled, the \texttt{RepeatedLetterFilter} addresses exaggerated letter repetitions that might arise from informal language or user typing quirks. Placing this filter early ensures that such noise does not affect subsequent expansion, folding, or duplicate removal.

\subsection{AbbreviationExpansionFilter (Optional)}
This filter expands known abbreviations using a predefined abbreviation map. 

In information retrieval and natural language processing, abbreviations are frequent sources of ambiguity. Expanding abbreviations to their full forms can enhance both search accuracy and linguistic analysis. This document describes the implementation logic of an abbreviation expansion filter in a Lucene-based text processing pipeline, detailing its steps and rationale.
For each token, it performs the following:
\begin{itemize}
    \item Checks if the lowercase form of the token exists in a provided abbreviation map.
    \item If found, replaces the token with its expanded form.
    \item Otherwise, leaves the token unchanged.
\end{itemize}


\paragraph{Case Insensitivity}

By converting tokens to lowercase before lookup, the filter accommodates abbreviations written in different cases (e.g., ``NLP'', ``nlp'', ``Nlp''), ensuring consistent expansion regardless of text casing.

\paragraph{Pipeline Integration}

Since the filter only modifies tokens when a match is found, it preserves the behavior of downstream filters, ensuring modular and reusable analysis components.


\subsection{ICUFoldingFilter}
The ICU folding filter is applied to harmonize Unicode representations, handling diacritics, special characters, and other script variations. Placing this filter at this stage ensures that downstream filters operate on a canonicalized representation.

\subsection{NBSPFilter}
The \texttt{NBSPFilter} normalizes non-breaking spaces and related whitespace anomalies that may affect token boundaries. Applying it after Unicode folding ensures consistent spacing.

\subsection{ElisionFilter}
This filter removes common French elisions (e.g., ``l'avion'' $\rightarrow$ ``avion''). It is applied after normalization steps to ensure that contractions and elided forms are treated as their base terms.

\subsection{RemoveDuplicatesTokenFilter}
Duplicate tokens are removed at this stage to avoid term inflation that could bias scoring or relevance models.

\subsection{CompoundPOSTokenFilter (Optional)}
In natural language processing (NLP), grouping or combining adjacent tokens based on their part-of-speech (POS) tags is a common technique to identify and process meaningful multi-word expressions (MWEs), compound nouns, and other linguistic constructs. This document presents a detailed explanation of the logic implemented in the provided token filter, which applies POS-based token grouping within a Lucene analysis pipeline.
The filter operates by:
\begin{itemize}
    \item Buffering all tokens from the input token stream.
    \item Assigning POS tags to the buffered tokens using a pre-trained POS tagger.
    \item Combining tokens based on specific POS tag patterns.
    \item Reconstructing a new token buffer containing the grouped tokens.
    \item Returning the next token on demand.
\end{itemize}

Initially, all tokens from the input stream are buffered. This ensures that the POS tagger can analyze the entire sentence or sequence, rather than processing tokens in isolation. This global view is essential for capturing meaningful patterns such as compound nouns or adjective-noun sequences.

\subsubsection{POS Tag Patterns for Grouping}

The core logic groups tokens based on a set of pre-defined POS tag patterns:
\begin{itemize}
    \item \textbf{Three-token patterns}:
    \begin{itemize}
        \item \texttt{NC-NC-NC}: Three consecutive common nouns (e.g., \textit{``carte bancaire dette''}).
        \item \texttt{NC-P-NC}: Noun + Preposition + Noun (e.g., \textit{``président de société ''}).
    \end{itemize}
    \item \textbf{Two-token patterns}:
    \begin{itemize}
        \item \texttt{NC-NC}, \texttt{N-N}: Consecutive nouns (e.g., \textit{``ingénieur logiciel''}).
        \item \texttt{V-NC}: Verb followed by a noun (e.g., \textit{``prendre pause''}).
        \item \texttt{ADJ-ADJ}: Consecutive adjectives (e.g., \textit{``grand rouge ''}).
        \item \texttt{NC-ADJ}: Noun followed by an adjective (e.g., \textit{``pomme verte''}).
    \end{itemize}
\end{itemize}

When a match is detected:
\begin{enumerate}
    \item The relevant tokens are concatenated using a hyphen (e.g., ``ingénieur logiciel'').
    \item The token type is set to \texttt{NN}, indicating a compound noun.
    \item The grouped token is added to the new buffer.
\end{enumerate}
If no pattern matches, the token is added as is, with its corresponding POS tag as its type.


The processing order ensures that:
\begin{enumerate}
    \item POS tagging occurs after all tokens are collected, ensuring correct context-aware tagging.
    \item Grouping is performed from the start of the sequence, respecting the natural left-to-right order of language.
    \item Longer groupings (three-token patterns) are prioritized over shorter ones to avoid prematurely splitting sequences that could form meaningful MWEs.
    \item The type attribute assignment (\texttt{NN}) standardizes the representation of multi-word expressions in downstream analysis.
\end{enumerate}

This token filter exemplifies an effective method for combining tokens based on syntactic criteria using POS tags. The design ensures that the resulting token stream captures meaningful multi-word expressions, enhancing the quality of text analysis and retrieval within a Lucene-based system.


\subsection{ShingleFilter (Optional)}
This filter generates n-grams (shingles) from the token stream. It is placed after POS-based filtering to capture meaningful multi-word units. Applying it before length or position filters ensures that n-grams are subject to subsequent size and position constraints.

\subsection{LengthFilter (Optional)}
This filter restricts tokens by minimum and maximum length. Placing it after n-gram generation ensures that composite tokens are also evaluated for length.

\subsection{Stopword Removal:} The \texttt{StopFilter}, loaded with a list of irrelevant tokens, removes common stopwords that do not contribute to the semantic meaning of texts and queries

\subsection{PositionFilter (Optional)}
Finally, the \texttt{PositionFilter} adjusts token position increments for downstream analysis or ranking. As the last step, it operates on the fully processed token stream.

The sequence of token filters in the \texttt{createComponents} method is carefully designed to ensure that each filter operates on a consistently normalized and preprocessed token stream. This design maximizes the effectiveness of each filter while preserving the linguistic and statistical integrity of the analysis pipeline.


\subsection{Stemming:} customised to handle French morphology:
    \begin{itemize}
        \item \texttt{SnowballFilter}(snow) - Employs the Snowball stemming algorithm, recognized for its effectiveness across languages.
        \item \texttt{FrenchLightStemFilter }  Applies a light stem, suitable for texts requiring minimal conversion.
        \item \texttt{FrenchMinimalStemFilter } Executes minimal stemming, avoiding over-reduction

\vspace{0.2cm}
\textbf{French Minimal Stemmer}  
The \textit{French Minimal Stemmer} is even simpler: it removes only a few suffixes, aiming to minimally transform words. It is ideal in contexts where retaining the original word root is crucial, but is less effective at reducing morphological variants.

    \end{itemize}

\subsection{Tokenization:}
    We offer a selection of tokenizer modules designed to accommodate diverse text processing requirements:
    \begin{itemize}
        \item \texttt{WhitespaceTokenizer} – This tokenizer segments text based on whitespace delimiters, proving optimal for inputs with inherent structural formatting.
        \item \texttt{LetterTokenizer} – It separates tokens at any non-alphabetic character, making it particularly suited for the analysis of purely alphabetic textual content.
        \item \texttt{StandardTokenizer} – This component utilizes sophisticated parsing rules to manage intricate textual constructs, including the proper handling of punctuation and special characters. 
        \item \texttt{OpenNLPTokenizer} – This option employs statistical models provided by OpenNLP for tokenization, a method highly advantageous for processing complex linguistic structures.
    \end{itemize}



\subsection{Indexer}

\begin{itemize}
    \item \textbf{BodyField}: represents the main content field of a document:
        \begin{itemize}
            \item \textbf{token}: the body text is split into tokens.
            \item \textbf{frequency}: the frequency of tokens is recorded.
            \item \textbf{position}: the positions of tokens are stored to support phrase queries.
            \item \textbf{body store}: the entire document body is stored to enable re-ranking.
        \end{itemize}
        
    \item \textbf{StartField}: 
        \begin{itemize}
            \item \textbf{token}: the start field is tokenized.
            \item \textbf{frequency}: the frequency of tokens in the start field is stored.
            \item \textbf{body store}: the initial portion of the document is saved.
        \end{itemize}
        
    \item \textbf{HighlightedField}: 
        \begin{itemize}
            \item \textbf{token}: the highlighted sections are tokenized.
            \item \textbf{frequency}: the frequency of tokens in the highlights is recorded.
            \item \textbf{body store}: the highlights of the document are stored.
        \end{itemize}
        
    \item \textbf{DirectoryIndexer}: This class is responsible for indexing documents within a specified directory. It supports several configuration parameters, such as the directory path, the parser type used to process documents, the analyzer for text processing, the similarity metric applied during indexing, the expected number of documents, and the location where the final index is saved.
\end{itemize}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Collection & Docs Size (GB) & Body Terms & Number of Documents & Index Size (GB) \\
        \hline
        French 2022-06 & 9,3 & 3,423,470 & 1,590,024 & 7.5 \\
        \hline
        French 2023-01 & 12.8 & 38,313,777 & 2,537,554 & 12.4 \\
        \hline
    \end{tabular}
    \caption{Indexing output details.}
    \label{tab:Indexing_details}
\end{table}

\subsubsection{StopList Choice}
Subsequent to the indexing of all documents within the training set, we utilized LUKE, a utility provided by Lucene. This tool enabled us to thoroughly inspect our indexed documents, gaining insights into their indexing structure and facilitating the identification of the most frequently occurring tokens within our collection.

In an effort to optimize performance, we proceeded to construct diverse stop lists. These lists incorporated various combinations, specifically differentiating between numerical terms, English linguistic terms, or a blend of both. We then rigorously analyzed the computational outcomes derived from each of these distinct configurations.

We chose the best one, namely "stopList40FR-10EN", composed by the first 40 French terms and 10 English terms. We decided to use it for all the runs.

\textbf{Stemmer tuning:}

\begin{table}[ht]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \begin{tabular}{ |l|c|c| }
            \hline
            Run name 2022-06 & nDCG & MAP \\ [0.5ex]
            \hline\hline
            stem-FrenchLight & 0.1398 & 0.0703 \\
            \rowcolor{yellow}
            stem-SnowBall & 0.1499 & 0.0767 \\
            stem-FrenchMinimal & 0.1426 & 0.0722 \\
            \hline
        \end{tabular}
        \caption{Stemmer tuning 2022-06}
        \label{tab:stemmer-2022}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \begin{tabular}{ |l|c|c| }
            \hline
            Run name 2023-01 & nDCG & MAP \\ [0.5ex]
            \hline\hline
            stem-FrenchLight & 0.2207 & 0.1345 \\
            \rowcolor{yellow}
            stem-SnowBall & 0.2315 & 0.1422 \\
            stem-FrenchMinimal & 0.2250 & 0.1370 \\
            \hline
        \end{tabular}
        \caption{Stemmer tuning 2023-01}
        \label{tab:stemmer-2023}
    \end{minipage}
\end{table}

\textbf{Tokenizer tuning:}

\begin{table}[ht]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \begin{tabular}{ |l|c|c| }
            \hline
            Run name 2022-06 & nDCG & MAP \\ [0.5ex]
            \hline\hline
            tokenizer-WhiteSpace & 0.1541 & 0.0796 \\
            \rowcolor{yellow}
            tokenizer-Letter & 0.1723 & 0.0923 \\
            tokenizer-Standard & 0.1710 & 0.0919 \\
            \hline
        \end{tabular}
        \caption{Tokenizer tuning 2022-06}
        \label{tab:tokenizer-2022}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \begin{tabular}{ |l|c|c| }
            \hline
            Run name 2023-01 & nDCG & MAP \\ [0.5ex]
            \hline\hline
            tokenizer-WhiteSpace & 0.2420 & 0.1512 \\
            \rowcolor{yellow}
            tokenizer-Letter & 0.2675 & 0.1710 \\
            tokenizer-Standard & 0.2656 & 0.1695 \\
            \hline
        \end{tabular}
        \caption{Tokenizer tuning 2023-01}
        \label{tab:tokenizer-2023}
    \end{minipage}
\end{table}

\subsection{Searcher}
\label{subsec:searcher}
Within the searcher component, we employed a Boolean query strategy to support query expansion and improve the overall performance of the information retrieval system. The initial query terms were marked as mandatory for document retrieval, while the expanded terms were added as optional elements, allowing for greater flexibility in matching relevant documents. Additionally, we applied a boosting mechanism to assign higher weight to the original query, ensuring it remained central to the retrieval process. The following section provides a detailed explanation of the implemented query expansion methodology.

\subsubsection{Start}
\label{subsubsec:Start}
It is often observed that the primary subject of a document is introduced at the beginning of the text. Based on this assumption, we designed the indexer to store the first five lines of each document. This design choice allowed the search engine to perform query operations specifically on this initial segment.

\subsubsection{Word N-grams}
\label{subsubsec:N-grams}
As the first step in our query expansion approach, we used word N-grams—sequences of consecutive terms extracted from the original query—to enrich the query representation. This process involves tokenizing the initial query into individual words and then recombining them into adjacent word pairs. For example, the query running shoes women is tokenized into running, shoes, and women, from which we generate the bigrams running shoes and shoes women.


\subsubsection{Fuzzy Search}
\label{subsubsec:fuzzy}
To enhance the robustness of our search system, we integrated a fuzzy search mechanism that supports approximate matches between query terms and document content. This technique is especially effective in addressing minor input errors, such as typos, letter transpositions, or common abbreviations. By setting a similarity threshold, the system ensures that only plausible variants of the query terms are considered, maintaining an acceptable level of accuracy. For example, a query like Adibas chaussures could successfully retrieve documents containing the correctly spelled brand name Adidas.

\subsubsection{Dictionary}
\label{subsubsec:Dictionary}
During the analysis of user input, we found that some queries appeared in unconventional formats, such as URLs, dot-separated terms (e.g., term1.term2.term3), or concatenated words without spaces (e.g., term1term2term3), which complicated processing by the search engine. To handle issues caused by punctuation, we implemented a regular-expression-based function to systematically remove these characters from the input.

For concatenated terms, we employed a dictionary derived from the provided synonyms file. This dictionary enables the system to identify and extract meaningful sub-terms embedded within single-word queries.

\subsubsection{Synonyms}
\label{subsubsec:synonyms}
Implementing synonyms in retrieval presents challenges, including semantic ambiguity and noise due to polysemy and irrelevant matches. While synonyms theoretically broaden the search scope, actual improvements depend on the relevance of the context. To manage computational overhead, synonyms were handled at query time using a query expansion model. This approach aims to improve retrieval accuracy while keeping the system efficient.

Synonyms Tuning: Since the synonyms approach randomly selects three synonyms from the collection, we decided not to apply the same experimental testing as we did with the Fuzzy and Phrase modules, due to the lack of repeatability introduced by the random selection process.

\subsubsection{Phrase Query}
\label{subsubsec:Phrase-Query}
To further improve the precision of our retrieval system, we implemented a Phrase Query Searcher that identifies documents containing an exact sequence of terms in the order they appear in the query. Phrase queries require that the specified terms occur in contiguous positions within the document.

\subsubsection{Word2Vec}
\label{subsubsec:Word2Vec}
To support word similarity functionality via a web-accessible interface, we developed a lightweight HTTP server using the Flask framework, chosen for its simplicity and ease of deployment in small-scale applications. At the core of the system is the KeyedVectors interface from the Gensim library, which allows efficient loading and querying of pre-trained word embedding models.

For our implementation, we used 300-dimensional French fastText word vectors (cc.fr.300.vec), trained on the large-scale Common Crawl corpus. These vectors are especially suitable for capturing semantic relationships between French words, thanks to their subword-based architecture that improves performance on rare or morphologically rich terms.

The main purpose of this service is to support query expansion within our information retrieval pipeline. When a user submits a search query, the system sends the main term to the /similar endpoint.

\subsubsection{LLM}
\label{subsubsec:LLM}
LLM-based query expansion is a technique used to improve the effectiveness of a search engine by automatically expanding the original user query. This approach is particularly useful when user input is too generic, vague, or ambiguous to return truly relevant results.

In our implementation, the query is sent to a Large Language Model (LLM), specifically ChatGPT-3.5-turbo, which interprets the meaning of the sentence and generates three to four related terms.

This number of terms was chosen strategically: too many would introduce information noise, potentially reducing precision, while too few would compromise the effectiveness of the technique.

The choice of using ChatGPT-3.5-turbo was driven by a trade-off between performance and cost.

\subsubsection{Query Construction}
\label{subsubsec:Query Construction}
The core search process employs a dynamic Boolean query builder with weighted clauses:

\begin{itemize}
    \item \textbf{Base Query}: Mandatory clause parsed from the original query string
    \item \textbf{Field Boosting}:
    \begin{itemize}
        \item Start field matches boosted by 1.5$\times$
        \item Highlight field matches boosted by 1.2$\times$
    \end{itemize}
    \item \textbf{Query Expansion}: Optional features:
    \begin{itemize}
        \item Dictionary-based synonyms (0.1$\times$ weight)
        \item Fuzzy search with edit distance=2 (0.2$\times$-0.8$\times$ weights), which allows for typos
        \item Phrase queries with slop=5 window
    \end{itemize}
\end{itemize}

\subsubsection{Query Expansion Techniques}
\label{subsubsec:Query Expansion Techniques}
We implemented six complementary expansion strategies:

\begin{itemize}
    \item \textbf{Word N-grams Generation}: Generates bigrams from query terms to capture word associations.
    \item \textbf{Synonym Expansion}: Uses dictionary-based synonyms with boosted weights:
    \begin{itemize}
        \item Start field matches boosted by 1.5$\times$
        \item Highlight field matches boosted by 1.2$\times$
    \end{itemize}
    \item \textbf{Neural Expansion}: 
    \begin{itemize}
    \item LLM-based term generation via GPT-3.5/DeepSeek APIs
    \item Requests 5-10 related terms per query
    \item Applies 0.4$\times$ weight to generated terms
    \end{itemize}
    \item \textbf{Pseudo-Relevance Feedback (PRF)}: Uses top-ranked documents to expand queries with relevant terms.
\end{itemize}

\subsubsection{Re-ranking Pipeline}
\label{subsubsec:Re-ranking Pipeline}
Final results undergo neural re-ranking:

\begin{enumerate}
    \item Normalize initial scores to [0,1] range
    \item Retrieve top 100 document bodies
    \item Invoke Cohere multilingual-v3 re-ranker
    \item Combine scores using linear interpolation:
    \[
    \mathit{finalScore} = 0.4 \cdot \mathit{BM25} + 0.6 \cdot \mathit{CohereScore}
    \]
\end{enumerate}





\subsubsection{Heuristic Reranker}


In the context of information retrieval systems, the \texttt{HeuristicReranker} module is designed to adjust the relevance scores of retrieved documents based on several heuristic penalties and bonuses. This aims to improve the ranking quality by applying domain-specific adjustments that consider document length, vocabulary richness, repetition, and potential spam.


The module extracts terms and their frequencies from Lucene term vectors:
\begin{itemize}
    \item \texttt{getDocumentTerms()} collects term frequencies for each document using \texttt{IndexReader} and stores them in a \texttt{Map}.
\end{itemize}

\noindent\textbf{\large Heuristic Penalties:} \\
Several penalties are applied to rerank documents:

\begin{itemize}
    \item \textbf{Length Penalty}: Penalizes documents that are too short or excessively long, to prioritize well-structured documents.
    \item \textbf{Vocabulary Penalty}: Assesses the lexical richness of a document (ratio of unique terms to total terms). Documents with low richness are penalized, while highly diverse documents receive a score bonus.
    \item \textbf{Spam Term Frequency Penalty}: Penalizes documents that exhibit keyword stuffing, i.e., where query terms are overrepresented relative to the total document length.
    \item \textbf{Repetition Penalty}: Penalizes documents with high term repetition, which might indicate low-quality or spammy content.
\end{itemize}

\noindent\textbf{\large Query Heuristics:} \\
The module also implements simple query heuristics:
\begin{itemize}
    \item For short queries (two terms or fewer), documents that are longer and lexically rich are boosted.
    \item For queries with diverse terms, documents with high lexical richness are also boosted.
\end{itemize}


\subsectionsection{Conclusion}
The \texttt{HeuristicReranker} module enhances search result relevance by applying domain-specific heuristics to document scores. This approach balances length, diversity, repetition, and spam detection to improve user satisfaction and ranking quality.
