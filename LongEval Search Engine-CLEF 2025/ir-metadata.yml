# We use the ir-metadata standard to describe and process runs.
# The fields below are mandatory, you can add additional metadata if you like.
# There are two libraries that automate and simplify tracking of experimental metadata that you can use optionally:
#
#   - The metadata module of [repro_eval](https://www.ir-metadata.org/software/):
#     Tracks the platform and implementation of your experiments.
#
#   - The [tirex_tracker](https://github.com/tira-io/tirex-tracker/):
#     Tracks the platform, implementation, and resources (e.g., CPU, GPU, RAM, emissions, etc.) of your experiments.
#
# See https://www.ir-metadata.org for more details.

tag: searchill_YYYY-MM

actor:
  # The name of the team
  team: Searchill

# Please provide a short description of your system.
research goal:
  description: |
     The goal of the research was to develop an effective and efficient search engine for the CLEF LongEval-Retrieval. 
     Our information retrieval system consists of three main phases: parsing, indexing, and searching. It accepts TREC files, cleaning the text by removing HTML tags, links, and special characters.
     During indexing, documents are converted into a searchable format using a customizable tokenizer and comparator. The search phase executes queries over the index, supporting tokenization. 
     Results are ranked and formatted for evaluation.
     The system is built for French-language collections, using a custom analyzer with French-English-specific stemming and stopword removal.
  evaluation:
     measures:
      - MAP
      - nDCG

platform:
  software:
    # Which software and tools did you use for training, tunning and running your system?
    # You can maintain the software that you used manually.
    # Alternatively, you can use repro_eval or the tirex_tracker to track this.
    libraries:
      - Lucene 9.10.0
      - org.apache.lucene:lucene-analyzers-icu - 8.11.2
      - org.apache.commons:commons-lang3 – 3.14.0
      - net.sourceforge.argparse4j:argparse4j – 0.9.0
      - com.fasterxml.jackson.core:jackson-core – 2.15.1
      - com.fasterxml.jackson.core:jackson-databind – 2.15.1
      - junit – 4.13.2
      - org.apache.opennlp:opennlp-tools – 2.1.0
      - com.cohere:cohere-java – 1.0.4
      - org.xerial:sqlite-jdbc – 3.49.1.0
      - com.vdurmont:emoji-java – 5.1.1

implementation:
  source:
    # Please provide a reference to your code if possible.
    # If you can not share your code, you can delete the implementation section.
    # The repro_eval and/or tirex_tracker tools can track this automatically, including commits, branches, etc.
    repository: https://bitbucket.org/upd-dei-stud-prj/seupd2425-searchill/src/master/

data:
  # Please describe which training data your system used, e.g., longeval-sci, longeval-web, MS MARCO, etc.
  training data:
    name: longeval-web
  other:
    name: stopword lists
    source: custom

method:
  # Boolean value indicating if it is a automatic (true) or manual (false) run
  automatic: true

  indexing:
    tokenizer: LetterTokenizer
    stemmer: None
    stopwords: stopList-40FR-10EN

  retrieval:
    - # Which ranking approach do you use? E.g., bm25
      name: BM25

      ##################################################
      # Yes/No Questions
      ##################################################

      # Did you use any statistical ranking model? (yes/no)
      lexical: yes

      # Did you use any deep neural network model? (yes/no)
      deep_neural_model: no

      # Did you use a sparse neural model? (yes/no):
      sparse_neural_model: no

      # Did you use a dense neural model? (yes/no):
      dense_neural_model: no

      # Did you use more than a single retrieval model? (yes/no):
      single_stage_retrieval: no

